{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T14:42:28.033823Z",
     "start_time": "2022-04-28T14:42:24.684664Z"
    },
    "id": "Hn-NzTm1vStX"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from numpy.core.fromnumeric import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T14:42:28.154376Z",
     "start_time": "2022-04-28T14:42:28.040102Z"
    },
    "id": "cws5CdVwvTxa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class maze:\n",
    "\n",
    "    '''creating the environment for maze-learning\n",
    "       possible actions - Down, Up, Left, Right\n",
    "    '''\n",
    "\n",
    "    def __init__(self, maze1):\n",
    "        self.maze1 = maze1\n",
    "        self.goal_state,self.start_state = self.goal()\n",
    "        self.og_maze1 = self.convert()\n",
    "        self.pos = self.start_state\n",
    "        self.actions = ['D','U','L','R']\n",
    "\n",
    "    def goal(self):\n",
    "        '''defining start and goal state'''\n",
    "        goal = 0\n",
    "        start = 0\n",
    "        for i in range(len(self.maze1)):\n",
    "            for j in range(len(self.maze1[0])):\n",
    "                if self.maze1[i][j] =='g' :\n",
    "                    goal = (i,j)\n",
    "                if self.maze1[i][j] =='s':\n",
    "                    start = (i,j)\n",
    "        return goal,start\n",
    "    \n",
    "    def possible_actions(self,pos1=None):\n",
    "        '''returning the possible actions at given state'''\n",
    "        \n",
    "        li = []\n",
    "        i,j = pos1 if pos1!=None else self.pos\n",
    "        if self.maze1[i][j] == 'h':\n",
    "            #print('Unknown state')\n",
    "            return -1\n",
    "        if 0<=i+1<len(self.maze1) and 0<=j<len(self.maze1[0]) and self.maze1[i+1][j]!='h':\n",
    "            li.append('D')\n",
    "        if 0<=i-1<len(self.maze1) and 0<=j<len(self.maze1[0]) and self.maze1[i-1][j]!='h':\n",
    "            li.append('U')\n",
    "        if 0<=i<len(self.maze1) and 0<=j-1<len(self.maze1[0]) and self.maze1[i][j-1]!='h':\n",
    "            li.append('L')\n",
    "        if 0<=i<len(self.maze1) and 0<=j+1<len(self.maze1[0]) and self.maze1[i][j+1]!='h':\n",
    "            li.append('R')\n",
    "        return li\n",
    "\n",
    "    def movement(self,a,pos1=None):\n",
    "        '''transition function returns the next position if valid action'''\n",
    "\n",
    "        i,j = pos1 if pos1!=None else self.pos\n",
    "        possible = self.possible_actions((i,j))\n",
    "        if a in possible:\n",
    "            if a == 'D':\n",
    "                self.pos = (i+1,j)\n",
    "            elif a == 'U':\n",
    "                self.pos = (i-1,j)\n",
    "            elif a == 'L':\n",
    "                self.pos = (i,j-1)\n",
    "            elif a == 'R':\n",
    "                self.pos = (i,j+1)\n",
    "            return self.pos\n",
    "        else:\n",
    "            print('Wrong action')\n",
    "\n",
    "    def reset(self):\n",
    "        '''reset the enviroment and agent'''\n",
    "\n",
    "        self.__init__(self.og_maze1)\n",
    "\n",
    "    def convert(self):\n",
    "        '''assigning the reward for start and goal state'''\n",
    "        temp = copy.deepcopy(maze1)\n",
    "        temp[self.goal_state[0]][self.goal_state[1]]=10\n",
    "        temp[self.start_state[0]][self.start_state[1]]=0\n",
    "        return temp\n",
    "\n",
    "\n",
    "#defining the maze for learning \n",
    "maze1 = [['s',0,0,0,0,0,0,'h',0,0,0],\n",
    "         [0,'h',0,'h',0,'h',0,'h','h','h',0],\n",
    "         [0,'h',0,'h',0,'h','h',0,0,0,0],\n",
    "         [0,'h',0,'h','h','h','h','h','h','h',0],\n",
    "         [0,'h',0,0,0,0,0,0,0,'h',0],\n",
    "         [0,'h','h','h','h','h',0,'h',0,'h',0],\n",
    "         [0,0,0,'h',0,0,0,'h',0,'h',0],\n",
    "         [0,'h','h','h',0,'h','h','h','h','h','h'],\n",
    "         [0,0,0,'h',0,'h',0,0,0,0,0],\n",
    "         ['h','h',0,'h',0,'h',0,'h','h','h','h'],\n",
    "         [0,0,0,'h',0,0,0,0,0,0,'g'],\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T14:42:30.061878Z",
     "start_time": "2022-04-28T14:42:28.163415Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "i2Y1mJJjZQ7F",
    "outputId": "b16a874a-9a7f-44cd-aad7-07268e66abb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-6d0fbffb1755>:25: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.axes().set_aspect('equal') #set the x and y axes to the same scale\n",
      "<ipython-input-3-6d0fbffb1755>:28: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.axes().invert_yaxis() #invert the y-axis so the first row of data is at the top\n",
      "<ipython-input-3-6d0fbffb1755>:29: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.axes().text(0.2, 0.5, 'S', style='normal',color= 'red' , fontweight= 'bold',fontsize = 20)\n",
      "<ipython-input-3-6d0fbffb1755>:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.axes().text(10.5, 10.75, 'G', style='normal',color= 'Green' , fontweight= 'bold',fontsize = 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates a graphical representation of the maze being used above\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "a = np.array([[0,0,0,0,0,0,0,1,0,0,0],\n",
    "         [0,1,0,1,0,1,0,1,1,1,0],\n",
    "         [0,1,0,1,0,1,0,0,0,0,0],\n",
    "         [0,1,0,1,1,1,1,1,1,1,0],\n",
    "         [0,1,0,0,0,0,0,0,0,1,0],\n",
    "         [0,1,1,1,1,1,0,1,0,1,0],\n",
    "         [0,0,0,1,0,0,0,1,0,1,0],\n",
    "         [0,1,1,1,0,1,1,1,1,1,1],\n",
    "         [0,0,0,1,0,1,0,0,0,0,0],\n",
    "         [1,1,0,1,0,1,0,1,1,1,1],\n",
    "         [0,0,0,1,0,0,0,0,0,0,0],\n",
    "         ])\n",
    "a[a == 0] = 2\n",
    "a[a== 1 ] = 0\n",
    "a[a==2] = 1\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "plt.pcolormesh(a)\n",
    "plt.axes().set_aspect('equal') #set the x and y axes to the same scale\n",
    "plt.xticks([]) # remove the tick marks by setting to an empty list\n",
    "plt.yticks([]) # remove the tick marks by setting to an empty list\n",
    "plt.axes().invert_yaxis() #invert the y-axis so the first row of data is at the top\n",
    "plt.axes().text(0.2, 0.5, 'S', style='normal',color= 'red' , fontweight= 'bold',fontsize = 20)\n",
    "plt.axes().text(10.5, 10.75, 'G', style='normal',color= 'Green' , fontweight= 'bold',fontsize = 20)\n",
    "plt.savefig('Maze.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6m1oZBA_XGi"
   },
   "source": [
    "#Model based Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T14:42:30.299423Z",
     "start_time": "2022-04-28T14:42:30.068001Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llkVRC-ZyA8_",
    "outputId": "c27d68ff-652a-4a58-e66f-479fce882752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward \n",
      " [[ 0.88629381  0.9847709   1.09418989  0.9847709   0.88629381  0.79766443\n",
      "   0.71789799  0.          0.          0.          0.        ]\n",
      " [ 0.79766443  0.          1.21576655  0.          0.79766443  0.\n",
      "   0.64610819  0.          0.          0.          0.        ]\n",
      " [ 0.71789799  0.          1.35085172  0.          0.71789799  0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.64610819  0.          1.50094635  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.58149737  0.          1.66771817  1.85302019  2.05891132  2.28767925\n",
      "   2.54186583  2.28767925  2.05891132  0.          0.        ]\n",
      " [ 0.52334763  0.          0.          0.          0.          0.\n",
      "   2.82429536  0.          1.85302019  0.          0.        ]\n",
      " [ 0.47101287  0.42391158  0.38152042  0.          3.87420489  3.4867844\n",
      "   3.13810596  0.          1.66771817  0.          0.        ]\n",
      " [ 0.42391158  0.          0.          0.          4.3046721   0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.38152042  0.34336838  0.30903154  0.          4.782969    0.\n",
      "   5.9049      5.31441     4.782969    4.3046721   3.87420489]\n",
      " [ 0.          0.          0.27812839  0.          5.31441     0.\n",
      "   6.561       0.          0.          0.          0.        ]\n",
      " [ 0.2027556   0.225284    0.25031555  0.          5.9049      6.561\n",
      "   7.29        8.1         9.         10.          0.        ]]\n",
      "\n",
      "\n",
      "Policy \n",
      " [['R', 'R', 'D', 'L', 'L', 'L', 'L', 'NA', 'R', 'L', 'D'], ['U', 'NA', 'D', 'NA', 'U', 'NA', 'U', 'NA', 'NA', 'NA', 'D'], ['U', 'NA', 'D', 'NA', 'U', 'NA', 'NA', 'R', 'L', 'L', 'D'], ['U', 'NA', 'D', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'D'], ['U', 'NA', 'R', 'R', 'R', 'R', 'D', 'L', 'L', 'NA', 'D'], ['U', 'NA', 'NA', 'NA', 'NA', 'NA', 'D', 'NA', 'U', 'NA', 'D'], ['U', 'L', 'L', 'NA', 'D', 'L', 'L', 'NA', 'U', 'NA', 'U'], ['U', 'NA', 'NA', 'NA', 'D', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA'], ['U', 'L', 'L', 'NA', 'D', 'NA', 'D', 'L', 'L', 'L', 'L'], ['NA', 'NA', 'U', 'NA', 'D', 'NA', 'D', 'NA', 'NA', 'NA', 'NA'], ['R', 'R', 'U', 'NA', 'R', 'R', 'R', 'R', 'R', 'R', 'NA']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-10\n",
    "gamma = 0.9\n",
    "iter = 100\n",
    "\n",
    "import copy\n",
    "\n",
    "### Used Value Iteration \n",
    "\n",
    "def reinforcement(m,iter, maze1, gamma, thresh):\n",
    "    maze1 = np.array(maze1)\n",
    "    x = maze1.shape\n",
    "    policy = [['NA' for i in range(x[1])] for i in range(x[0])]\n",
    "    rewards = np.zeros(x)\n",
    "\n",
    "    for _ in range(iter):\n",
    "        old_reward = copy.deepcopy(rewards)\n",
    "        for i in range(len(rewards)):\n",
    "            for j in range((len(rewards[0]))):\n",
    "                temp = m.possible_actions((i,j))\n",
    "                if temp ==-1 or (i,j)==m.goal_state:\n",
    "                    continue\n",
    "                total = []    \n",
    "                for a in temp:\n",
    "                    s_next = m.movement(a,(i,j))\n",
    "                    #print(i,j,s_next,a)\n",
    "                    total.append(m.og_maze1[s_next[0]][s_next[1]] + gamma*old_reward[s_next[0]][s_next[1]])\n",
    "                policy[i][j] = temp[argmax(total)]\n",
    "                rewards[i][j] = max(total)\n",
    "        #print(rewards)\n",
    "        if np.all(np.abs(old_reward-rewards) < thresh):\n",
    "            return rewards,policy\n",
    "    return rewards,policy\n",
    "\n",
    "m = maze(maze1)\n",
    "reward,policy = reinforcement(m,iter,maze1,gamma,threshold)\n",
    "print('Reward \\n', reward)\n",
    "print('\\n\\nPolicy \\n', policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0qAb8ziACFi"
   },
   "source": [
    "#Model free learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T14:42:30.360583Z",
     "start_time": "2022-04-28T14:42:30.307076Z"
    },
    "id": "0fHhCay-PkkE"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "### Q Learing Algorithm\n",
    "\n",
    "def init_table(m,maze1,x=0):\n",
    "    ''' initializes the q-table '''\n",
    "\n",
    "    maze1 = np.array(maze1)\n",
    "    states = [[x*1.0,x*1.0,x*1.0,x*1.0] for i in range(maze1.shape[0]*maze1.shape[1])]\n",
    "    states = np.array(states)\n",
    "    total = set(m.actions)\n",
    "    \n",
    "    #possible actions\n",
    "    move ={'U':0, 'D':1 , 'L':2 , 'R':3 }\n",
    "    for i in range(len(states)):\n",
    "        x,y = i//maze1.shape[1],i%maze1.shape[1]\n",
    "        li = m.possible_actions((x,y))\n",
    "        if li != -1:\n",
    "            li = set(li)\n",
    "        else:\n",
    "            li = set()\n",
    "        current = total - li\n",
    "        for j in current:\n",
    "            states[i,move[j]] = -999\n",
    "    \n",
    "    x,y = m.goal_state\n",
    "    current = x*maze1.shape[1] + y\n",
    "    #initializes the goal state q-table value\n",
    "    states[current] = np.array([10.0,10.0,10.0,10.0])\n",
    "    return states\n",
    "\n",
    "\n",
    "def extract(states):\n",
    "    ''' extract policy from q table values'''\n",
    "    li = []\n",
    "    rev_move = {0:'U', 1:'D' ,2:'L' , 3:'R' }\n",
    "    for i in states:\n",
    "        temp = rev_move[argmax(i)]\n",
    "        if max(i) in [-999,0,10]:\n",
    "            li.append('NA')\n",
    "        else:\n",
    "            li.append(temp)\n",
    "    return li\n",
    "\n",
    "def q_learning(m,maze1,episodes,c,epsilon=0.1):\n",
    "    maze1 = np.array(maze1)\n",
    "    shapes = maze1.shape\n",
    "    steps = 0\n",
    "    if c==0:\n",
    "      states = init_table(m,maze1)\n",
    "      \n",
    "    else:\n",
    "      states = init_table(m,maze1,10)\n",
    "    move ={'U':0, 'D':1 , 'L':2 , 'R':3 }\n",
    "    rev_move = {0:'U', 1:'D' ,2:'L' , 3:'R' }\n",
    "    for i in range(episodes):\n",
    "        pos = m.start_state\n",
    "        current = pos[0]*shapes[1] + pos[1]\n",
    "        final = m.goal_state[0]*shapes[1] + m.goal_state[1]\n",
    "        old_q = copy.deepcopy(states)\n",
    "    \n",
    "        while current != final:\n",
    "\n",
    "            ## Strategy to choose action for current state\n",
    "            p = random.uniform(0,1)\n",
    "            if p <= epsilon:\n",
    "                a = rev_move[argmax(states[current])]    \n",
    "            else:\n",
    "                a = random.sample(m.possible_actions(pos),1)[0]\n",
    "\n",
    "            ## PErform action\n",
    "            next_state = m.movement(a,pos)\n",
    "            temp = next_state[0]*shapes[1] + next_state[1]\n",
    "            steps += 1\n",
    "            ## update Q Table\n",
    "            states[current,move[a]] = states[current,move[a]] + 0.1*((m.og_maze1[next_state[0]][next_state[1]]) + 0.95*(max(states[temp])) - states[current,move[a]])\n",
    "            \n",
    "            ## Next Iteration values\n",
    "            pos = next_state\n",
    "            current = pos[0]*shapes[1] + pos[1]\n",
    "            \n",
    "            \n",
    "        if np.all(np.abs(old_q - states) < 1e-20):\n",
    "            print('No of Episodes :',i,'\\nTotal Steps :',steps)\n",
    "            ##Extract Policy\n",
    "            policy = extract(states)\n",
    "            return states,policy,steps/i\n",
    "    \n",
    "    \n",
    "    policy = extract(states)\n",
    "    return states,policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T14:42:38.792191Z",
     "start_time": "2022-04-28T14:42:30.362616Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXkA6IzcQJRX",
    "outputId": "f3f60213-49a0-4792-c2ca-6b0c6147f259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Episodes : 384 \n",
      "Total Steps : 144094\n",
      "8.413131952285767\n"
     ]
    }
   ],
   "source": [
    "## Q Table initialized to 0\n",
    "import time\n",
    "start = time.time()\n",
    "states1, policy1,q1 = q_learning(m,maze1,500,0)\n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T14:42:46.374676Z",
     "start_time": "2022-04-28T14:42:38.799636Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4t6gtn99hUrs",
    "outputId": "8d76e6fd-33d4-4dc4-c2b6-17db9e9ef0cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Episodes : 384 \n",
      "Total Steps : 130890\n",
      "7.557901382446289\n"
     ]
    }
   ],
   "source": [
    "## Q Table Initialized to 10\n",
    "import time\n",
    "start = time.time()\n",
    "states1, policy1,q2 = q_learning(m,maze1,500,1)\n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUpJXRgeDvQ1"
   },
   "source": [
    "# DynaQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T14:42:46.420862Z",
     "start_time": "2022-04-28T14:42:46.382976Z"
    },
    "id": "qlo4-8RnV8RO"
   },
   "outputs": [],
   "source": [
    "## To store already seen state action pairs and thier corresponding next state and reward\n",
    "\n",
    "class Model():\n",
    "  def __init__(self, n_states, n_actions):\n",
    "    self.transitions = np.zeros((n_states,n_actions), dtype=np.uint8)\n",
    "    self.rewards = np.zeros((n_states, n_actions))\n",
    "\n",
    "  def add(self,s,a,s_prime,r):\n",
    "    ## Stores next state and reward for each seen state-action pairs\n",
    "    self.transitions[s,a] = s_prime\n",
    "    self.rewards[s,a] = r\n",
    "\n",
    "  def sample(self):\n",
    "    \"\"\" Return random state, action\"\"\"\n",
    "    # Random visited state\n",
    "    s = np.random.choice(np.where(np.sum(self.transitions, axis=1) > 0)[0])\n",
    "    # Random action in that state\n",
    "    a = np.random.choice(np.where(self.transitions[s] > 0)[0])\n",
    "\n",
    "    return s,a\n",
    "\n",
    "  def step(self, s,a):\n",
    "    \"\"\" Return state_prime and reward for state-action pair\"\"\"\n",
    "    s_prime = self.transitions[s,a]\n",
    "    r = self.rewards[s,a]\n",
    "    return s_prime, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T14:42:46.478076Z",
     "start_time": "2022-04-28T14:42:46.427439Z"
    },
    "id": "5dTZn3mRV8RO"
   },
   "outputs": [],
   "source": [
    "def planning(n,model,states):\n",
    "  \"\"\" Reinforces the previously learnt state-action pairs\"\"\"\n",
    "\n",
    "  for i in range(n):\n",
    "  \n",
    "    s, a = model.sample()\n",
    "    nexts , reward = model.step(s,a)\n",
    "  \n",
    "    states[s,a] = states[s,a] + 0.9*(reward + 0.95*(max(states[nexts])) - states[s,a])\n",
    "  \n",
    "    return states\n",
    "\n",
    "\n",
    "def samples(i):\n",
    "    a = max(i)\n",
    "    c = np.where(i==a)[0]\n",
    "    return random.sample(list(c),1)[0]\n",
    "\n",
    "\n",
    "def dynaq(m,maze1,episodes,n,epsilon = 0.75):\n",
    "  \n",
    "  real_steps=0\n",
    "  planning_steps = 0\n",
    "\n",
    "  maze1 = np.array(maze1)\n",
    "  shapes = maze1.shape\n",
    "\n",
    "  states = init_table(m,maze1)## Initializes Q Table\n",
    "\n",
    "  move ={'U':0, 'D':1 , 'L':2 , 'R':3 }\n",
    "  rev_move = {0:'U', 1:'D' ,2:'L' , 3:'R' }\n",
    "\n",
    "  model = Model(shapes[0]*shapes[1],4)\n",
    "\n",
    "  for i in range(episodes):\n",
    "\n",
    "    pos = m.start_state\n",
    "    current = pos[0]*shapes[1] + pos[1]\n",
    "\n",
    "    final = m.goal_state[0]*shapes[1] + m.goal_state[1]\n",
    "    old_q = copy.deepcopy(states)\n",
    "\n",
    "    while current!=final :\n",
    "\n",
    "      ## Strategy to choose action for current state\n",
    "      p = random.uniform(0,1)\n",
    "      if p <= epsilon:\n",
    "          a = rev_move[samples(states[current])]    \n",
    "      else:\n",
    "          a = random.sample(m.possible_actions(pos),1)[0]\n",
    "      \n",
    "      ## Next State based on action\n",
    "      next_state = m.movement(a,pos)\n",
    "      temp = next_state[0]*shapes[1] + next_state[1]\n",
    "\n",
    "      real_steps += 1\n",
    "      planning_steps += n\n",
    "\n",
    "      #Updates Q Table\n",
    "      states[current,move[a]] = states[current,move[a]] + 0.1*((m.og_maze1[next_state[0]][next_state[1]]) + 0.95*(max(states[temp])) - states[current,move[a]])\n",
    "\n",
    "      ## Adds observed enviornment data to model and reinforces data seen till now\n",
    "      model.add(current,move[a],temp,m.og_maze1[next_state[0]][next_state[1]])\n",
    "      states = planning(n,model,states)\n",
    "      \n",
    "      ##Next Iteration values\n",
    "      pos = next_state\n",
    "      current = pos[0]*shapes[1] + pos[1]\n",
    "\n",
    "  ## Extracts Policy\n",
    "  policy = extract(states)\n",
    "  \n",
    "  return real_steps/episodes,planning_steps/episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T14:42:24.804Z"
    },
    "id": "4dbeeQ0gYKDS"
   },
   "outputs": [],
   "source": [
    "## Runs DynaQ with different no of planning steps to observe agent performance\n",
    "\n",
    "import time\n",
    "t = np.zeros(40)\n",
    "rs = np.zeros(40)\n",
    "ps = np.zeros(40)\n",
    "ts = np.zeros(40)\n",
    "for j in range(10):\n",
    "  for i in range(1,41):\n",
    "    s = time.time()\n",
    "    r,p = dynaq(m,maze1,50,i)\n",
    "    t[i-1]+=(time.time()-s)\n",
    "    rs[i-1]+=r\n",
    "    ps[i-1]+=p\n",
    "    ts[i-1]+=(r+p)\n",
    " \n",
    "t/=10\n",
    "rs/=10\n",
    "ps/=10\n",
    "ts/=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lit6l2NeGDLw"
   },
   "source": [
    "# MBPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T14:42:24.814Z"
    },
    "id": "v6ijKG7Myw9E"
   },
   "outputs": [],
   "source": [
    "##Calculates Pseudorewards using Bounded Real Time Dynamic Programming\n",
    "\n",
    "def brtdp(m,iter, maze1, gamma):\n",
    "    maze1 = np.array(maze1)\n",
    "    x = maze1.shape\n",
    "    vtable = np.zeros(x) ##Initialize table to 0\n",
    "    vtable[x[0]-1][x[1]-1] = 10 ##Initialize goal state to 10\n",
    "    \n",
    "    policy = [['NA' for j in range(11)] for i in range(11)] ##Stores Policy at the end of n iterations\n",
    "    \n",
    "    if(iter==0):\n",
    "      return np.zeros(maze1.shape),policy\n",
    "    \n",
    "    ## Similar to Value Iteration\n",
    "    for k in range(iter):\n",
    "        old_reward = copy.deepcopy(vtable)\n",
    "        old_policy = copy.deepcopy(policy)\n",
    "        for i in range(len(vtable)):\n",
    "            for j in range((len(vtable[0]))):\n",
    "                temp = m.possible_actions((i,j))\n",
    "                if temp ==-1 or (i,j)==m.goal_state:\n",
    "                    continue\n",
    "                total = []    \n",
    "                for a in temp:\n",
    "                    s_next = m.movement(a,(i,j))\n",
    "                    \n",
    "                    total.append(m.og_maze1[s_next[0]][s_next[1]] + gamma*old_reward[s_next[0]][s_next[1]])\n",
    "                vtable[i][j] = max(total)\n",
    "                policy[i][j] = temp[argmax(total)]\n",
    "   \n",
    "        \n",
    "       \n",
    "    return vtable, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T14:42:24.820Z"
    },
    "id": "uOdzACr9YS93"
   },
   "outputs": [],
   "source": [
    "## initializes Q table with pseudorewards using policy calculated above\n",
    "\n",
    "def upstates(states,pseudorewards,policy,shapes):\n",
    "  move ={'U':0, 'D':1 , 'L':2 , 'R':3 }\n",
    "  for i in range(shapes[0]):\n",
    "    for j in range(shapes[1]):\n",
    "      if policy[i][j]!= 'NA':\n",
    "        states[i*shapes[1]+j][move[policy[i][j]]]= pseudorewards[i][j]\n",
    "  return states\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T14:42:24.824Z"
    },
    "id": "BvwBBdNM0Yck"
   },
   "outputs": [],
   "source": [
    "## Model Based PseudoReward Approximation Algorithm\n",
    "\n",
    "def mbpa(m,maze1,episodes,n,epsilon=0.1):\n",
    "    maze1 = np.array(maze1)\n",
    "    shapes = maze1.shape\n",
    "    steps = 0\n",
    "\n",
    "    pseudorewards, policybest = brtdp(m,n,maze1,0.95) ##Calculates Pseudoreewards and Policy\n",
    "    \n",
    "    ## Initializes Q Table\n",
    "    states = init_table(m,maze1)\n",
    "    states = upstates(states,pseudorewards,policybest,shapes)\n",
    "    \n",
    "    move ={'U':0, 'D':1 , 'L':2 , 'R':3 }\n",
    "    rev_move = {0:'U', 1:'D' ,2:'L' , 3:'R' }\n",
    "    \n",
    "    ## Same Procedure as Q learning with small addition to reward function. New Reward function is Reward + PseudoReward\n",
    "    for i in range(episodes):\n",
    "        pos = m.start_state\n",
    "        step = 0\n",
    "        current = pos[0]*shapes[1] + pos[1]\n",
    "        final = m.goal_state[0]*shapes[1] + m.goal_state[1]\n",
    "        old_q = copy.deepcopy(states)\n",
    "        while current != final and step<2000:\n",
    "            p = random.uniform(0,1)\n",
    "            if p <= epsilon:\n",
    "                a = rev_move[np.random.choice(list(np.where(states[current]==max(states[current]))[0]),1)[0]]    \n",
    "            else:\n",
    "                a = random.sample(m.possible_actions(pos),1)[0]\n",
    "            next_state = m.movement(a,pos)\n",
    "            temp = next_state[0]*shapes[1] + next_state[1]\n",
    "            steps += 1\n",
    "            states[current,move[a]] = states[current,move[a]] + 0.1*((m.og_maze1[next_state[0]][next_state[1]]) +  pseudorewards[current//shapes[1]][current%shapes[1]] + 0.95*(max(states[temp])) - states[current,move[a]])\n",
    "            #print(a,m.og_maze1[next_state[0]][next_state[1]],pos,next_state,states[current,move[a]])\n",
    "            pos = next_state\n",
    "            current = pos[0]*shapes[1] + pos[1]\n",
    "   \n",
    "    policy = extract(states)\n",
    "    return states,policy,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T14:42:24.832Z"
    },
    "id": "2wTKTE921QxN"
   },
   "outputs": [],
   "source": [
    "## Runs MBPA with different no of bellman updates to observe agent performance\n",
    "\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "s = np.zeros(41)\n",
    "mbpa1t = np.zeros(41)\n",
    "maze1 = [['s',0,0,0,0,0,0,'h',0,0,0],\n",
    "         [0,'h',0,'h',0,'h',0,'h','h','h',0],\n",
    "         [0,'h',0,'h',0,'h','h',0,0,0,0],\n",
    "         [0,'h',0,'h','h','h','h','h','h','h',0],\n",
    "         [0,'h',0,0,0,0,0,0,0,'h',0],\n",
    "         [0,'h','h','h','h','h',0,'h',0,'h',0],\n",
    "         [0,0,0,'h',0,0,0,'h',0,'h',0],\n",
    "         [0,'h','h','h',0,'h','h','h','h','h','h'],\n",
    "         [0,0,0,'h',0,'h',0,0,0,0,0],\n",
    "         ['h','h',0,'h',0,'h',0,'h','h','h','h'],\n",
    "         [0,0,0,'h',0,0,0,0,0,0,'g'],\n",
    "         ]\n",
    "for j in range(10):\n",
    "  for i in range(0,41):\n",
    "    m = maze(maze1)\n",
    "    start = time.time()\n",
    "    states1, policy1,steps = mbpa(m,maze1,50,i,0.75)\n",
    "    mbpa1t[i]+=(time.time() - start)\n",
    "    s[i]+=(steps/50)\n",
    "\n",
    "s/=10\n",
    "mbpa1t/=10\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T14:42:24.839Z"
    },
    "id": "5BwZ0eLLnF0M"
   },
   "outputs": [],
   "source": [
    "## Initializes Value Table to max value of 10\n",
    "\n",
    "def conv(x,vtable):\n",
    "  for i in range(len(vtable)):\n",
    "    for j in range(len(vtable[i])):\n",
    "      if(vtable[i][j]==0):\n",
    "        vtable[i][j] == x\n",
    "  return vtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T14:42:24.845Z"
    },
    "id": "OzMpWWmYi3rN"
   },
   "outputs": [],
   "source": [
    "def brtdp2(m,iter, maze1, gamma):\n",
    "    policy = [['NA' for j in range(11)] for i in range(11)]\n",
    "    if(iter==0):\n",
    "      return np.zeros(maze1.shape),policy\n",
    "    maze1 = np.array(maze1)\n",
    "    x = maze1.shape\n",
    "    vtable = copy.deepcopy(m.og_maze1)\n",
    "    vtable = conv(10,vtable)\n",
    "\n",
    "    \n",
    "    for k in range(iter):\n",
    "        old_reward = copy.deepcopy(vtable)\n",
    "        old_policy = copy.deepcopy(policy)\n",
    "        for i in range(len(vtable)):\n",
    "            for j in range((len(vtable[0]))):\n",
    "                temp = m.possible_actions((i,j))\n",
    "                if temp ==-1 or (i,j)==m.goal_state:\n",
    "                    continue\n",
    "                total = []    \n",
    "                for a in temp:\n",
    "                    s_next = m.movement(a,(i,j))\n",
    "                    \n",
    "                    total.append(m.og_maze1[s_next[0]][s_next[1]] + gamma*old_reward[s_next[0]][s_next[1]])\n",
    "                vtable[i][j] = max(total)\n",
    "                policy[i][j] = temp[argmax(total)]\n",
    "    np.array(policy).reshape((11,11))\n",
    "    ##print(policy)\n",
    "    return vtable,policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T14:42:24.850Z"
    },
    "id": "XXSzQ0-wjOQU"
   },
   "outputs": [],
   "source": [
    "def mbpa(m,maze1,episodes,n,epsilon=0.1):\n",
    "    maze1 = np.array(maze1)\n",
    "    shapes = maze1.shape\n",
    "    steps = 0\n",
    "    pseudorewards , bestpolicy = brtdp2(m,n,maze1,0.95)\n",
    "    states = init_table(m,maze1)\n",
    "    states = upstates(states,pseudorewards,bestpolicy,shapes)\n",
    "    move ={'U':0, 'D':1 , 'L':2 , 'R':3 }\n",
    "    rev_move = {0:'U', 1:'D' ,2:'L' , 3:'R' }\n",
    "    for i in range(episodes):\n",
    "        pos = m.start_state\n",
    "        current = pos[0]*shapes[1] + pos[1]\n",
    "        final = m.goal_state[0]*shapes[1] + m.goal_state[1]\n",
    "        old_q = copy.deepcopy(states)\n",
    "        step = 0\n",
    "        while current != final and step<2000:\n",
    "            p = random.uniform(0,1)\n",
    "            if p <= epsilon:\n",
    "                a = rev_move[np.random.choice(list(np.where(states[current]==max(states[current]))[0]),1)[0]]    \n",
    "            else:\n",
    "                a = random.sample(m.possible_actions(pos),1)[0]\n",
    "            next_state = m.movement(a,pos)\n",
    "            temp = next_state[0]*shapes[1] + next_state[1]\n",
    "            steps += 1\n",
    "            states[current,move[a]] = states[current,move[a]] + 0.1*((m.og_maze1[next_state[0]][next_state[1]]) + pseudorewards[current//shapes[1]][current%shapes[1]] + 0.95*(max(states[temp])) - states[current,move[a]])\n",
    "            \n",
    "            pos = next_state\n",
    "            current = pos[0]*shapes[1] + pos[1]\n",
    "   \n",
    "    policy = extract(states)\n",
    "    return states,policy,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T14:42:24.857Z"
    },
    "id": "4tlQA7OujAMW"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "s2 = np.zeros(41)\n",
    "mbpa2t = np.zeros(41)\n",
    "maze1 = [['s',0,0,0,0,0,0,'h',0,0,0],\n",
    "         [0,'h',0,'h',0,'h',0,'h','h','h',0],\n",
    "         [0,'h',0,'h',0,'h','h',0,0,0,0],\n",
    "         [0,'h',0,'h','h','h','h','h','h','h',0],\n",
    "         [0,'h',0,0,0,0,0,0,0,'h',0],\n",
    "         [0,'h','h','h','h','h',0,'h',0,'h',0],\n",
    "         [0,0,0,'h',0,0,0,'h',0,'h',0],\n",
    "         [0,'h','h','h',0,'h','h','h','h','h','h'],\n",
    "         [0,0,0,'h',0,'h',0,0,0,0,0],\n",
    "         ['h','h',0,'h',0,'h',0,'h','h','h','h'],\n",
    "         [0,0,0,'h',0,0,0,0,0,0,'g'],\n",
    "         ]\n",
    "for j in range(10):\n",
    "  for i in range(0,41):\n",
    "    m = maze(maze1)\n",
    "    start = time.time()\n",
    "    states1, policy1,steps = mbpa(m,maze1,50,i,0.75)\n",
    "    mbpa2t[i]+=(time.time() - start)\n",
    "    s2[i]+=(steps/50)\n",
    "\n",
    "s2/=10\n",
    "mbpa2t/=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T14:42:24.863Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "PmEHfKSWMsvQ",
    "outputId": "01c946a5-d1c1-445f-b764-416baf675efb"
   },
   "outputs": [],
   "source": [
    "## Plots Steps per episode taken by various algorithm above \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "\n",
    "plt.plot(s2[0],label = 'Q learning Upper' , marker = '*' ,markersize = 15,COLOR='RED')\n",
    "plt.plot(s[0],label = 'Q learning Lower' , marker = '*' ,markersize = 15, COLOR=\"BLUE\")\n",
    "plt.plot([i for i in range(1,41)],s[1:],label='MBPA LOWER',COLOR='BLUE')\n",
    "plt.plot([i for i in range(1,41)],s2[1:],label='MBPA UPPER',COLOR='RED')\n",
    "plt.plot([i for i in range(1,41)],rs[0:],label='Dyna Real Steps',COLOR=\"GREEN\")\n",
    "plt.plot([i for i in range(1,41)],ps[0:],label='Dyna Planning Steps',COLOR=\"YELLOW\")\n",
    "plt.plot([i for i in range(1,41)],ts[0:],label='Dyna Total Steps',COLOR='PURPLE')\n",
    "plt.legend()\n",
    "plt.xlabel('No of Bellman Updates / No of Planning steps per Real step')\n",
    "plt.ylabel('Mean No of Steps per Episode')\n",
    "plt.ylim([0,2500])\n",
    "plt.savefig('Maze Steps.png')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T14:42:24.867Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "ykNt29195foN",
    "outputId": "7d62044e-1439-4e05-ee9a-016eddaeaf52"
   },
   "outputs": [],
   "source": [
    "##PLots time taken to converge to optimal policy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "plt.plot([i for i in range(1,41)],mbpa1t[1:],label='MBPA LOWER',COLOR='BLUE')\n",
    "plt.plot(mbpa2t[0],label = 'Q learning Upper' , marker = '*' ,markersize = 15,COLOR='RED')\n",
    "plt.plot(mbpa1t[0],label = 'Q learning Lower' , marker = '*' ,markersize = 15, COLOR=\"BLUE\")\n",
    "plt.plot([i for i in range(1,41)],mbpa2t[1:],label='MBPA UPPER',COLOR='RED')\n",
    "plt.plot([i for i in range(1,41)],t[0:],label='Dyna ',color = 'Green')\n",
    "plt.xlabel('No of Bellman Updates / No of Planning steps per Real step')\n",
    "plt.ylabel('Time taken to learn Shortest path')\n",
    "plt.legend()\n",
    "plt.savefig('MAze Time.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RL_maze_pc funal - Copy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
