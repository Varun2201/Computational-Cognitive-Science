{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T15:31:12.621507Z",
     "start_time": "2022-04-28T15:31:11.892732Z"
    },
    "id": "QGiQ8d_qP9EX"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import copy \n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.core.fromnumeric import argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wp2W2-DdEwuH"
   },
   "source": [
    "# Working Code From Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T15:31:11.897Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3u6GTzRS4sM",
    "outputId": "d236c03b-8b67-4372-f56a-45b51c529177"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " For 1 Planning steps\n",
      "\n",
      "Episode 100 Average Reward: -843.04\n",
      "Episode 200 Average Reward: -409.68\n",
      "Episode 300 Average Reward: -284.39\n",
      "Episode 400 Average Reward: -265.09\n",
      "Episode 500 Average Reward: -212.94\n",
      "\n",
      " For 6 Planning steps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "\n",
    "class Model():\n",
    "  def __init__(self):\n",
    "    self.transitions = [[[0 for i in range(3)] for j in range(15)] for k in range(19)]\n",
    "    self.rewards = np.zeros((19,15,3))\n",
    "\n",
    "  def add(self,s,a,s_prime,r):\n",
    "    self.transitions[s[0]][s[1]][a] = s_prime\n",
    "    self.rewards[s[0],s[1],a] = r\n",
    "\n",
    "  def sample(self):\n",
    "    \"\"\" Return random state, action\"\"\"\n",
    "    # Random visited state\n",
    "    x = np.where(self.rewards!=0)\n",
    "    c = np.random.randint(0,len(x[0]))\n",
    "    pos = x[0][c]\n",
    "    vel = x[1][c]\n",
    "    a   = x[2][c]\n",
    "    ##print(pos,vel,a,self.transitions[pos][vel][a])\n",
    "    return (pos,vel),a\n",
    "\n",
    "  def step(self, s,a):\n",
    "    \"\"\" Return state_prime and reward for state-action pair\"\"\"\n",
    "    s_prime = self.transitions[s[0]][s[1]][a]\n",
    "    r = self.rewards[s[0],s[1],a]\n",
    "\n",
    "    return s_prime, r\n",
    "\n",
    "def planning(n,model,states):\n",
    "  for i in range(n):\n",
    "    ##print(model)\n",
    "    s, a = model.sample()\n",
    "    nexts , reward = model.step(s,a)\n",
    "    ##print(s,a,nexts,reward)\n",
    "    states[s[0],s[1],a] = states[s[0],s[1],a] + 0.01*(reward + 0.95*(np.max(states[nexts[0],nexts[1]])) - states[s[0],s[1],a])\n",
    "    #print(states)\n",
    "  return states\n",
    "\n",
    "\n",
    "\n",
    "def dynaqcar(env, learning, discount, episodes,n):\n",
    "    \n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*np.array([10, 100])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "    \n",
    "    \n",
    "    #Q = np.random.uniform(low = -1, high = 1, size = (num_states[0], num_states[1],env.action_space.n))\n",
    "    Q = np.zeros((num_states[0], num_states[1], env.action_space.n))\n",
    "    \n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    \n",
    "    m = Model()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(episodes):\n",
    "        \n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        steps = 0\n",
    "        epsilon = 0.01 * 0.99**(i)\n",
    "       \n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "        old_q = copy.deepcopy(Q)\n",
    "        \n",
    "        while tot_reward>-1000:   \n",
    "           \n",
    "            steps+=1\n",
    "\n",
    "            if i >= (episodes):\n",
    "                env.render()\n",
    "                \n",
    "            \n",
    "            if np.random.random() > epsilon:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "            \n",
    "            state2, reward, _, info = env.step(action) \n",
    "            \n",
    "            \n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            \n",
    "            \n",
    "            if  state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                tot_reward += 0\n",
    "                done = True\n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                delta = learning*(reward + discount*np.max(Q[state2_adj[0], state2_adj[1]]) - Q[state_adj[0], state_adj[1],action])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "                tot_reward+=-1                    \n",
    "            \n",
    "            \n",
    "            m.add(state_adj,action,state2_adj,reward)\n",
    "            Q = planning(n,m,Q)\n",
    "            state_adj = state2_adj\n",
    "        \n",
    "\n",
    "        reward_list.append(tot_reward)\n",
    "       \n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "            \n",
    "        if (i+1) % 100 == 0:    \n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "        \n",
    "        \n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return ave_reward_list\n",
    "\n",
    "\n",
    "rs = []\n",
    "ps = []\n",
    "ts = []\n",
    "td  = []\n",
    "for j in range(1,52,5):\n",
    "    s = time.time()\n",
    "    print('\\n For' , j , 'Planning steps\\n')\n",
    "    rewards = dynaqcar(env, 0.1, 0.95, 500,j)\n",
    "    st = np.mean(rewards)\n",
    "    td.append(time.time()-s)\n",
    "    rs.append(-1*st)\n",
    "    ps.append(-1*st*j)\n",
    "    ts.append(-1*st*(j+1))\n",
    "   \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T15:31:11.902Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKchTYykjDOH",
    "outputId": "922b3877-d60f-422b-b478-2796fe751960"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def reinforcement(env,n, gamma, thresh):\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env = env.unwrapped\n",
    "    env.reset()\n",
    "    rewards = np.zeros((19,15))\n",
    "    policy = np.zeros((19,15))\n",
    "    for _ in range(n):\n",
    "        old_reward = copy.deepcopy(rewards)\n",
    "        for i in range(19):\n",
    "            for j in range(15):\n",
    "                total = []\n",
    "                curr = np.round(np.array([i,j])/np.array([10, 100]) + env.observation_space.low,2)\n",
    "                \n",
    "                for a in range(3):\n",
    "                    env.reset()\n",
    "                    env.state = curr\n",
    "                    s_next,reward,done,info = env.step(a)\n",
    "\n",
    "                    ##print(curr,s_next,reward,done)\n",
    "                    s_next = (s_next - env.observation_space.low)*np.array([10, 100])\n",
    "                    s_next = np.round(s_next, 0).astype(int)\n",
    "                    if done:\n",
    "                        total.append(10)\n",
    "                    else :\n",
    "                        total.append(gamma*old_reward[s_next[0]][s_next[1]])\n",
    "                ##print(total)\n",
    "\n",
    "                rewards[i][j] = np.max(total)\n",
    "                if np.all(np.abs(rewards[i][j]-total)<1e-3):\n",
    "                    policy[i][j] = int(np.random.randint(0,3))\n",
    "                    rewards[i][j] = total[int(policy[i][j])]\n",
    "                else:    \n",
    "                    policy[i][j] = argmax(total)\n",
    "        #print(rewards)\n",
    "        '''if np.all(np.abs(old_reward-rewards) < thresh):\n",
    "            print('Cutoff')\n",
    "            return rewards'''\n",
    "    return rewards,policy\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env = env.unwrapped\n",
    "env.reset()\n",
    "\n",
    "reinforcement(env,35,0.8,1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T15:31:11.909Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAz5br-MjLnI",
    "outputId": "a7576a07-c7eb-4480-8c1a-ca31fec819ee",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "\n",
    "def upstates(states,pseudorewards,policy,shapes):\n",
    "  for i in range(shapes[0]):\n",
    "    for j in range(shapes[1]):\n",
    "      if policy[i][j]!= 'NA':\n",
    "        states[i][j][int(policy[i][j])]= pseudorewards[i][j]\n",
    "  return states\n",
    "\n",
    "def mbpa(env, learning, discount, episodes,value):\n",
    "\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*np.array([10, 100])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "\n",
    "    Q = np.zeros((num_states[0], num_states[1], env.action_space.n))\n",
    " \n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "\n",
    "    new_reward, policy = reinforcement(env,value,0.95,1e-10)\n",
    "\n",
    "    Q = upstates(Q,new_reward,policy,(19,15))\n",
    "\n",
    "    for i in range(episodes):\n",
    "       \n",
    "        done = False\n",
    "        steps = 0\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        epsilon = 0.01 * 0.99**(i)\n",
    "        old_q = copy.deepcopy(Q)\n",
    "       \n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "    \n",
    "        while done != True:   \n",
    "   \n",
    "            if np.random.random() >= epsilon:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "            \n",
    "            state2, reward, _, info = env.step(action) \n",
    "            \n",
    "            \n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            \n",
    "            \n",
    "            if  state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = 0\n",
    "                tot_reward += 0\n",
    "                done = True\n",
    "            else:\n",
    "                reward = reward + new_reward[state2_adj[0], state2_adj[1]]\n",
    "                delta = learning*(reward + discount*np.max(Q[state2_adj[0], state2_adj[1]]) - Q[state_adj[0], state_adj[1],action])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "                tot_reward += -1  \n",
    "            \n",
    "            \n",
    "            state_adj = state2_adj\n",
    "        \n",
    "       \n",
    "        reward_list.append(tot_reward)\n",
    "       \n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "            \n",
    "        if (i+1) % 100 == 0:    \n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "\n",
    "        \n",
    "            \n",
    "    env.close()\n",
    "    print(np.round(np.mean(ave_reward_list)*1,3))\n",
    "    return ave_reward_list\n",
    "\n",
    "step = []\n",
    "tmbpa = []\n",
    "for j in range(0,51,5):\n",
    "    s = time.time()\n",
    "    print('\\n For' , j , 'Bellman Updates\\n')\n",
    "    rewards = mbpa(env, 0.1, 0.95, 500,j)\n",
    "    step.append(-1*np.mean(rewards))\n",
    "    tmbpa.append(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T15:31:11.912Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "-yzE4nGTIoI2",
    "outputId": "8cd9d7a7-4037-41f7-9c83-4ad4761157ba"
   },
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(1,52,5)],ps[0:],label = 'DYNA PLANNING STEPS')\n",
    "plt.plot([i for i in range(1,52,5)],rs[0:],label = 'DYNA REAL STEPS')\n",
    "plt.plot([i for i in range(1,52,5)],ts[0:],label = 'DYNA TOTAL STEPS')\n",
    "plt.xlabel('No of Planning steps per Real step')\n",
    "plt.legend()\n",
    "plt.ylim(bottom = 0)\n",
    "plt.ylabel('Mean No of Steps per Episode')\n",
    "plt.savefig('MountainCarDynaSteps.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T15:31:11.915Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "eSeuXzPNmtpi",
    "outputId": "c7998342-b35f-4d04-ae0d-6d55037ad8c4"
   },
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(1,52,5)], step[0:],label = 'MBPA',color = 'red')\n",
    "plt.plot(step[0],marker = '*', markersize = 10, color = 'red',label = 'Q Learning')\n",
    "plt.plot([i for i in range(1,52,5)],rs[0:],label = 'DYNA REAL STEPS')\n",
    "plt.xlabel('No of Bellman Updates / No of Planning Steps')\n",
    "plt.legend()\n",
    "plt.ylabel('Mean No of Steps per Episode')\n",
    "plt.savefig('MountainCarSteps.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-28T15:31:11.918Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot([i for i in range(1,52,5)], tmbpa[0:],label = 'MBPA',color = 'red')\n",
    "plt.plot(tmbpa[0],marker = '*', markersize = 10, color = 'red',label = 'Q Learning')\n",
    "plt.plot([i for i in range(1,52,5)],td[0:],label = 'DYNAQ')\n",
    "plt.xlabel('No of Bellman Updates/ No of Planning Steps')\n",
    "plt.legend()\n",
    "plt.ylabel('Time')\n",
    "plt.savefig('MountainCarTime.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mountaincar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
